{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "from cutting_plane_framework import CuttingPlaneMethod\n",
    "import sys\n",
    "sys.path.append('./code_LY')\n",
    "from model import var_sorter\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NaiveCuttingPlaneTreeAlgorithm(CuttingPlaneMethod):\n",
    "    def __init__(self, instanceName, maxIteration=100, \n",
    "                    OutputFlag=0, Threads=1, MIPGap=0.0, TimeLimit=3600, MIPFocus=2, cglp_OutputFlag=0, \n",
    "                        cglp_Threads=1, cglp_MIPGap=0.0, cglp_TimeLimit=100, cglp_MIPFocus=0, \n",
    "                            addCutToMIP=False, number_branch_var=2, normalization='SNC', \n",
    "                                training=True, load_ckp=True, save_interval=20\n",
    "                            ):\n",
    "        super().__init__(instanceName, maxIteration, OutputFlag, Threads, MIPGap, TimeLimit, MIPFocus, cglp_OutputFlag, cglp_Threads, cglp_MIPGap, cglp_TimeLimit, cglp_MIPFocus, addCutToMIP, number_branch_var, normalization)\n",
    "        self.tensorA = None\n",
    "        self.col_feature = None\n",
    "        self.row_feature = None\n",
    "        # model\n",
    "        self.pred=var_sorter(v_size=6,c_size=3,sample_sizes=[64],multi_head=2,natt=2)\n",
    "        # optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.pred.parameters(),lr=1e-4)\n",
    "        # training params\n",
    "        self.training = training\n",
    "        self.lastLogit = None\n",
    "        self.save_interval = save_interval\n",
    "        if not os.path.isdir('./models'): # create model folder if there is no models folder\n",
    "            os.mkdir('./models')\n",
    "        self.ckp_starter = 0\n",
    "        if load_ckp: # checkpoint: load model and optimizer; start from the latest checkpoint\n",
    "            '''get latest checkpoint'''\n",
    "            #  resume model training from the latest checkpoint, allowing the training to continue from the state it was in when interrupted, rather than starting over from the beginning.\n",
    "            ckps = os.listdir('./models') # get all ckps (files with .mdl extension) in this directory\n",
    "            if len(ckps) > 0:\n",
    "                ckps.sort(key=lambda x:int(x.replace('.mdl','').split('_')[-1]))\n",
    "                tar_name = f'./models/{ckps[-1]}'\n",
    "                # load\n",
    "                cpu_dev = torch.device('cpu')\n",
    "                checkpoint = torch.load(tar_name,map_location=cpu_dev)  # load model and optimizer\n",
    "                self.pred.load_state_dict(checkpoint['model'])          # load model\n",
    "                self.ckp_starter = checkpoint['nepoch'] \n",
    "                print(f'Loaded check point: {tar_name}')\n",
    "            else:\n",
    "                print('No check point to load')\n",
    "        #set up log files\n",
    "        if not os.path.isdir('./logs'):\n",
    "            os.mkdir('./logs')\n",
    "        self.log_file = open('./logs/training.log','w') # 'w' means: if the file exists, it will be overwritten\n",
    "            \n",
    "    \n",
    "    def A_to_sparse_tensor(self):\n",
    "        self.tensorA = self.A.tocoo()\n",
    "        indices = np.vstack((self.tensorA.row,self.tensorA.col))\n",
    "        data = self.tensorA.data\n",
    "        self.tensorA = torch.sparse_coo_tensor(indices, data, self.tensorA.shape, dtype=torch.float32)\n",
    "        \n",
    "    def feat_extract(self):\n",
    "        #A matrix update\n",
    "        self.A_to_sparse_tensor()\n",
    "\n",
    "        #variable feature update\n",
    "        if self.col_feature is None:\n",
    "            #first time constructing \n",
    "            self.col_feature = [None] * len(self.variables)\n",
    "            for var in self.variables:   \n",
    "                rel_sol_indx= self.varName_map_position[var.VarName]\n",
    "                col_value = self.lp_sol[rel_sol_indx]\n",
    "                # if abs(col_val-round(col_val,0))>1e-6 and (vtp=='B' or vtp=='I'):\n",
    "                #     cand.append(col_var.VarName)\n",
    "                \n",
    "                #compose features\n",
    "                isBin=0\n",
    "                isInt=0\n",
    "                if var.VType == 'I':\n",
    "                    isInt=1\n",
    "                elif var.VType == 'B':\n",
    "                    isBin=1\n",
    "                # ---- [lp_sol, LB, UB, isBin, isInt, reduced_cost, lp_obj]\n",
    "                self.col_feature[rel_sol_indx] = [col_value, self.LB[rel_sol_indx], self.UB[rel_sol_indx], isBin, isInt, var.Obj]  \n",
    "            self.col_feature=torch.as_tensor(self.col_feature, dtype=torch.float32)  \n",
    "        else:\n",
    "            # not first time, update bound and col_val\n",
    "            for var in self.variables:   \n",
    "                rel_sol_indx = self.varName_map_position[var.VarName]\n",
    "                col_val = self.lp_sol[rel_sol_indx]\n",
    "                self.col_feature[rel_sol_indx][0] = col_val\n",
    "                \n",
    "        #constraint feature update\n",
    "        row_index_map={}\n",
    "        rcounter=0\n",
    "        row_feat=[]\n",
    "        for idx, constr in enumerate(self.lp_relaxation.getConstrs()):\n",
    "            if constr.ConstrName not in row_index_map:\n",
    "                row_index_map[constr.ConstrName] = rcounter\n",
    "                rcounter += 1\n",
    "            #compose row feature\n",
    "            sense1 = 0\n",
    "            sense2 = 0 # 01:leq 00:eq 10:geq\n",
    "            if constr.Sense == '<':\n",
    "                sense2 = 1\n",
    "            elif constr.Sense == '>':\n",
    "                sense1 = 1 \n",
    "            row_feat.append([constr.RHS, sense1, sense2]) # [RHS, SENSE]\n",
    "        row_feat = torch.as_tensor(row_feat, dtype=torch.float32)\n",
    "        cand = torch.as_tensor([self.varName_map_position[x] for x in self.Cand])\n",
    "        return  row_feat, row_index_map, cand\n",
    "        \n",
    "    \n",
    "    def variable_selection(self, ifTrain = True, variable_selection_way = 'MFR'):\n",
    "        # choose the variable to branch\n",
    "        #TODO: add criteria to select either explore or exploit\n",
    "        # when we will do heuristic branching, when we will explore, and when we will exploit?\n",
    "        if variable_selection_way == 'RL':\n",
    "            # the number of variables that are chosen to branch, so the number of nodes in the branching tree is 2^number_of_candidates            \n",
    "            number_of_candidates = self.number_branch_var  \n",
    "            # get feature, candidates are stored in self.Cand\n",
    "            row_feat, row_index_map, cand = self.feat_extract()\n",
    "            \n",
    "            # calling model to do prediction\n",
    "            if ifTrain:\n",
    "                self.optimizer.zero_grad()\n",
    "            self.lastLogit = self.pred(self.tensorA, self.col_feature, row_feat)\n",
    "            decision = torch.argsort(torch.index_select(self.lastLogit, 0, cand),descending=True)[:number_of_candidates]\n",
    "            self.branchVar[self.iteration-1] = {}\n",
    "            for i in decision:\n",
    "                tar_var = self.Cand[i]\n",
    "                # ori_ind = cand[i].item()\n",
    "                self.branchVar[self.iteration-1][tar_var] = self.lastLogit[i] # self.lp_sol[ori_ind]\n",
    "        elif variable_selection_way == 'MFR':\n",
    "            # heuristic: Maximum Fractionality Rule\n",
    "            number_of_candidates = self.number_branch_var                                                                       # the number of variables that are chosen to branch, so the number of nodes in the branching tree is 2^number_of_candidates\n",
    "            number_of_noninteger = len(self.non_integer_vars[self.iteration - 1])\n",
    "            number_of_nonbinary = len(self.non_binary_vars[self.iteration - 1])\n",
    "            self.branchVar[self.iteration-1] = {}\n",
    "            if number_of_noninteger > 0:\n",
    "                list1 = sorted(self.non_integer_vars[self.iteration-1].items(), key=lambda x: x[1], reverse=True)[:number_of_candidates]   # find the integer variables that have the largest distance to the nearest integer\n",
    "                if len(list1) <= number_of_candidates:\n",
    "                    for item in list1:\n",
    "                        self.branchVar[self.iteration-1][item[0]] = item[1] # \n",
    "                else:\n",
    "                    for item in list1[0:number_of_candidates]:\n",
    "                        self.branchVar[self.iteration-1][item[0]] = item[1]\n",
    "\n",
    "            if number_of_nonbinary > 0: \n",
    "                list2 = sorted(self.non_binary_vars[self.iteration-1].items(), key=lambda x: x[1], reverse=True)[:number_of_candidates]    # find the binary variables that have the largest distance to {0,1}\n",
    "                if len(list2) <= number_of_candidates:\n",
    "                    for item in list2:\n",
    "                        self.branchVar[self.iteration-1][item[0]] = item[1]\n",
    "                else:\n",
    "                    for item in list2[0:number_of_candidates]:\n",
    "                        self.branchVar[self.iteration-1][item[0]] = item[1]\n",
    "        \n",
    "\n",
    "\n",
    "    def branching_tree_building(self, node, level, varInfo):\n",
    "        if level == len(self.branchVar[self.iteration-1]):\n",
    "            return\n",
    "        else:\n",
    "            varName, info = list(varInfo.items())[level]\n",
    "            pos = self.varName_map_position[varName]\n",
    "\n",
    "            left_node = {}\n",
    "            left_node['LB'] = deepcopy(self.nodeSet[node]['LB'])\n",
    "            left_node['LB'][pos] = info['upper']\n",
    "            left_node['UB'] = deepcopy(self.nodeSet[node]['UB'])\n",
    "            left_node['trace'] = deepcopy(self.nodeSet[node]['trace'])\n",
    "            left_node['trace'].append('l')\n",
    "            \n",
    "            right_node = {}\n",
    "            right_node['LB'] = deepcopy(self.nodeSet[node]['LB'])\n",
    "            right_node['UB'] = deepcopy(self.nodeSet[node]['UB'])\n",
    "            right_node['UB'][pos] = info['lower']\n",
    "            right_node['trace'] = deepcopy(self.nodeSet[node]['trace'])\n",
    "            right_node['trace'].append('r')\n",
    "\n",
    "            left_node_ind = max(self.nodeSet.keys()) + 1\n",
    "            right_node_ind = left_node_ind + 1\n",
    "            self.nodeSet[left_node_ind] = left_node\n",
    "            self.nodeSet[right_node_ind] = right_node\n",
    "            del self.nodeSet[node]  \n",
    "\n",
    "            self.branching_tree_building(left_node_ind, level+1, varInfo)\n",
    "            self.branching_tree_building(right_node_ind, level+1, varInfo)\n",
    "     \n",
    "    def branching_tree(self):\n",
    "        varInfo = {}\n",
    "        for varName in self.branchVar[self.iteration-1].keys():\n",
    "            varInfo[varName] = {}\n",
    "            varInfo[varName]['val'] = self.lp_relaxation.getVarByName(varName).x\n",
    "            varInfo[varName]['lower'] = math.floor(varInfo[varName]['val'])\n",
    "            varInfo[varName]['upper'] = math.ceil(varInfo[varName]['val'])\n",
    "        \n",
    "        self.nodeSet = {}\n",
    "        self.nodeSet[0] = {}\n",
    "        self.nodeSet[0]['LB'] = deepcopy(self.LB)\n",
    "        self.nodeSet[0]['UB'] = deepcopy(self.UB)\n",
    "        self.nodeSet[0]['trace'] = []\n",
    "        self.branching_tree_building(0, 0, varInfo) \n",
    "    \n",
    "    def solve(self, ifTrain = True, variable_selection_way = 'RL'):\n",
    "        time_init = time.time()\n",
    "        while self.iteration <= self.maxIteration:\n",
    "            iter_begin = time.time()\n",
    "            self.master_problem()\n",
    "            if self.OPT == True:\n",
    "                self.print_iteration_info()\n",
    "                return\n",
    "            self.variable_selection(ifTrain, variable_selection_way)\n",
    "            self.branching_tree()\n",
    "            ready_to_cut = time.time()\n",
    "            self.cut_generation()\n",
    "            iter_end = time.time()\n",
    "            overall = iter_end - time_init\n",
    "            iteration_time = iter_end - iter_begin\n",
    "            cut_time = iter_end - ready_to_cut\n",
    "            self.print_iteration_info(cut_time, iteration_time, overall)\n",
    "\n",
    "\n",
    "    def train_model_each_iteration(self):\n",
    "        time_init = time.time()\n",
    "        while self.iteration <= self.maxIteration:\n",
    "            iter_begin = time.time()\n",
    "            self.master_problem()\n",
    "            if self.OPT == True:\n",
    "                self.print_iteration_info()\n",
    "                return\n",
    "            self.variable_selection(True, 'RL')\n",
    "            self.branching_tree()\n",
    "            ready_to_cut = time.time()\n",
    "            self.cut_generation()\n",
    "            iter_end = time.time()\n",
    "            overall = iter_end - time_init\n",
    "            iteration_time = iter_end - iter_begin\n",
    "            cut_time = iter_end - ready_to_cut\n",
    "            self.print_iteration_info(cut_time, iteration_time, overall)\n",
    "            if self.iteration > 1 and self.training:\n",
    "                # compute improvement\n",
    "                improvement = (self.lp_obj_value[self.iteration - 1] - self.lp_obj_value[self.iteration - 2]) / self.lp_obj_value[0] * 100\n",
    "                # record reward\n",
    "                self.log_file.write(f'iter:{self.iteration} reward:{improvement}\\n')\n",
    "                # update gradient and update model\n",
    "                regret = torch.sum(self.lastLogit * ((-1.0) * improvement + 1e-6))\n",
    "                regret.backward()\n",
    "                self.optimizer.step()\n",
    "                # check if need to save model\n",
    "                if (self.iteration-1) % self.save_interval == 0 and self.iteration-1 != 0:\n",
    "                    state = {'model':self.pred.state_dict(),'optimizer':self.optimizer.state_dict(),'nepoch':self.ckp_starter+self.iteration-1}\n",
    "                    torch.save(state,f'./models/ckp_{self.iteration+self.ckp_starter-1}.mdl')\n",
    "    \n",
    "    def train_model_each_round(self, num_episodes = 500):\n",
    "        #TODO: train model after each round, not each iteration; and for each round, we need to initialize the model (cuts, variables, etc.):\n",
    "        '''         # Initialize the model:\n",
    "                    self.iteration = 0\n",
    "                    self.lp_relaxation = self.mipModel.relax()\n",
    "                    self.lp_relaxation.update()\n",
    "                    self.coefList = {}\n",
    "                    self.lp_obj_value = {}\n",
    "        '''\n",
    "        for i in range(10):\n",
    "            with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n",
    "                for i_episode in range(int(num_episodes / 10)):\n",
    "                    time_init = time.time()\n",
    "                    self.iteration = 0\n",
    "                    self.lp_relaxation = self.mipModel.relax()\n",
    "                    self.lp_relaxation.update()\n",
    "                    self.coefList = {}\n",
    "                    self.lp_obj_value = {}\n",
    "                    while self.iteration <= self.maxIteration:\n",
    "                        iter_begin = time.time()\n",
    "                        self.master_problem()\n",
    "                        self.variable_selection()\n",
    "                        self.branching_tree()\n",
    "                        ready_to_cut = time.time()\n",
    "                        self.cut_generation()\n",
    "                        iter_end = time.time()\n",
    "                        overall = iter_end - time_init\n",
    "                        iteration_time = iter_end - iter_begin\n",
    "                        cut_time = iter_end - ready_to_cut\n",
    "                        self.print_iteration_info(cut_time, iteration_time, overall)\n",
    "                        \n",
    "                    # compute improvement\n",
    "                    improvement = abs(self.lp_obj_value[self.iteration - 1] - self.lp_obj_value[0]) / self.lp_obj_value[0] * 100\n",
    "                    # record reward\n",
    "                    self.log_file.write(f'episode:{i * 10 + i_episode + 1} reward:{improvement}\\n')\n",
    "                    # update gradient and update model\n",
    "                    regret = torch.tensor(-improvement, requires_grad=True)\n",
    "                    regret.backward()\n",
    "                    self.optimizer.step()\n",
    "                    # check if need to save model\n",
    "                    # if (self.iteration-1) % self.save_interval == 0 and self.iteration-1 != 0:\n",
    "                    #     state = {'model':self.pred.state_dict(),'optimizer':self.optimizer.state_dict(),'nepoch':self.ckp_starter+self.iteration-1}\n",
    "                    #     torch.save(state,f'./models/ckp_{self.iteration+self.ckp_starter-1}.mdl')\n",
    "                    # pbar.update(1)        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read MPS format model from file benchmark/50v-10.mps.gz\n",
      "Reading time = 0.03 seconds\n",
      "50v-10: 233 rows, 2013 columns, 2745 nonzeros\n",
      "Loaded check point: ./models/ckp_500.mdl\n",
      "This problem has 1647 integer variables and 0 binary variables.\n",
      "The optimal value of LP relaxation is 2879.0656868536717.\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "|  Iter  |  # fractional var  |  current value  |  Relative Improvement  |  Overall Improvement  |  Iter Time  |  Overall Time  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "|       1|                  30|        2882.7344|                 0.1273 |                0.1274 |      1.1397 |         1.8842 |\n",
      "|       2|                  30|        2885.5635|                 0.0980 |                0.2257 |      1.2072 |         3.0915 |\n",
      "|       3|                  29|        2888.0303|                 0.0854 |                0.3114 |      0.9080 |         3.9995 |\n",
      "|       4|                  29|        2893.0303|                 0.1728 |                0.4850 |      1.2293 |         5.2290 |\n",
      "|       5|                  29|        2901.4303|                 0.2895 |                0.7768 |      1.2518 |         6.4808 |\n",
      "|       6|                  29|        2904.2303|                 0.0964 |                0.8741 |      1.0969 |         7.5777 |\n",
      "|       7|                  29|        2906.7303|                 0.0860 |                0.9609 |      1.2088 |         8.7866 |\n",
      "|       8|                  29|        2910.0240|                 0.1132 |                1.0753 |      1.2407 |        10.0274 |\n",
      "|       9|                  30|        2910.8693|                 0.0290 |                1.1047 |      1.1923 |        11.2197 |\n",
      "|      10|                  30|        2913.7131|                 0.0976 |                1.2034 |      1.1831 |        12.4029 |\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "instanceName = '50v-10'\n",
    "cpt = NaiveCuttingPlaneTreeAlgorithm(instanceName,training=True, save_interval=20, maxIteration=100, OutputFlag = 0, Threads = 1, MIPGap = 0.0, TimeLimit = 300, number_branch_var = 2, normalization = 'SNC')\n",
    "cpt.solve(variable_selection_way = 'MFR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read MPS format model from file benchmark/50v-10.mps.gz\n",
      "Reading time = 0.03 seconds\n",
      "50v-10: 233 rows, 2013 columns, 2745 nonzeros\n",
      "Loaded check point: ./models/ckp_500.mdl\n",
      "This problem has 1647 integer variables and 0 binary variables.\n",
      "The optimal value of LP relaxation is 2879.0656868536717.\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "|  Iter  |  # fractional var  |  current value  |  Relative Improvement  |  Overall Improvement  |  Iter Time  |  Overall Time  |\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "|       1|                  29|        2887.4657|                 0.2909 |                0.2918 |      1.1777 |         2.0126 |\n",
      "|       2|                  29|        2887.4657|                 0.0000 |                0.2918 |      1.1169 |         3.1966 |\n",
      "|       3|                  29|        2892.4657|                 0.1729 |                0.4654 |      0.9977 |         4.2555 |\n",
      "|       4|                  28|        2898.2990|                 0.2013 |                0.6680 |      1.1994 |         5.5114 |\n",
      "|       5|                  28|        2898.2990|                 0.0000 |                0.6680 |      1.0918 |         6.6660 |\n",
      "|       6|                  28|        2903.9657|                 0.1951 |                0.8649 |      1.5146 |         8.2426 |\n",
      "|       7|                  32|        2906.2190|                 0.0775 |                0.9431 |      1.2865 |         9.5834 |\n",
      "|       8|                  33|        2906.9945|                 0.0267 |                0.9701 |      1.0393 |        10.6699 |\n",
      "|       9|                  32|        2907.2233|                 0.0079 |                0.9780 |      1.1339 |        11.8579 |\n",
      "|      10|                  31|        2907.2472|                 0.0008 |                0.9788 |      1.1478 |        13.0582 |\n",
      "|      11|                  31|        2913.4972|                 0.2145 |                1.1959 |      1.1210 |        14.2234 |\n",
      "|      12|                  34|        2919.0933|                 0.1917 |                1.3903 |      1.0977 |        15.3731 |\n",
      "|      13|                  33|        2919.1710|                 0.0027 |                1.3930 |      1.2575 |        16.6862 |\n",
      "|      14|                  36|        2920.0746|                 0.0309 |                1.4244 |      1.3257 |        18.0566 |\n",
      "|      15|                  39|        2920.5813|                 0.0174 |                1.4420 |      0.9415 |        19.0407 |\n",
      "|      16|                  35|        2921.3611|                 0.0267 |                1.4691 |      1.8648 |        20.9543 |\n",
      "|      17|                  36|        2924.1961|                 0.0970 |                1.5675 |      1.2583 |        22.2652 |\n",
      "|      18|                  38|        2924.4736|                 0.0095 |                1.5772 |      2.0864 |        24.3949 |\n",
      "|      19|                  36|        2924.5942|                 0.0041 |                1.5814 |      1.1335 |        25.5725 |\n",
      "|      20|                  31|        2924.6972|                 0.0035 |                1.5849 |      0.9072 |        26.5275 |\n",
      "|      21|                  32|        2925.0606|                 0.0124 |                1.5976 |      0.9563 |        27.5484 |\n",
      "|      22|                  32|        2927.3907|                 0.0796 |                1.6785 |      0.8926 |        28.4825 |\n",
      "|      23|                  32|        2928.6375|                 0.0426 |                1.7218 |      1.1007 |        29.6280 |\n",
      "|      24|                  32|        2930.0480|                 0.0481 |                1.7708 |      1.1652 |        30.8356 |\n",
      "|      25|                  32|        2933.3418|                 0.1123 |                1.8852 |      0.9354 |        31.8192 |\n",
      "|      26|                  32|        2934.7965|                 0.0496 |                1.9357 |      1.2007 |        33.0658 |\n",
      "|      27|                  32|        2936.4420|                 0.0560 |                1.9929 |      1.0486 |        34.1609 |\n",
      "|      28|                  33|        2936.9593|                 0.0176 |                2.0108 |      1.0544 |        35.2592 |\n",
      "|      29|                  32|        2938.3901|                 0.0487 |                2.0605 |      1.0720 |        36.3744 |\n",
      "|      30|                  33|        2940.8020|                 0.0820 |                2.1443 |      1.1204 |        37.5357 |\n",
      "|      31|                  35|        2941.0242|                 0.0076 |                2.1520 |      1.3590 |        38.9411 |\n",
      "|      32|                  32|        2941.0443|                 0.0007 |                2.1527 |      1.3394 |        40.3259 |\n",
      "|      33|                  33|        2941.6523|                 0.0207 |                2.1739 |      1.2900 |        41.6606 |\n",
      "|      34|                  33|        2943.0023|                 0.0459 |                2.2207 |      1.2645 |        42.9617 |\n",
      "|      35|                  34|        2945.8526|                 0.0968 |                2.3197 |      1.1734 |        44.1690 |\n",
      "|      36|                  36|        2945.8610|                 0.0003 |                2.3200 |      1.2512 |        45.4643 |\n",
      "|      37|                  33|        2945.8680|                 0.0002 |                2.3203 |      2.1392 |        47.6470 |\n",
      "|      38|                  34|        2947.5852|                 0.0583 |                2.3799 |      1.6726 |        49.3665 |\n",
      "|      39|                  37|        2947.7968|                 0.0072 |                2.3873 |      1.3175 |        50.7284 |\n",
      "|      40|                  40|        2948.3891|                 0.0201 |                2.4078 |      2.1602 |        52.9340 |\n",
      "|      41|                  41|        2948.4689|                 0.0027 |                2.4106 |      1.2110 |        54.2039 |\n",
      "|      42|                  41|        2948.6332|                 0.0056 |                2.4163 |      1.5406 |        55.7854 |\n",
      "|      43|                  36|        2948.8628|                 0.0078 |                2.4243 |      1.0885 |        56.9221 |\n",
      "|      44|                  36|        2950.4378|                 0.0534 |                2.4790 |      1.1592 |        58.1217 |\n",
      "|      45|                  36|        2951.1128|                 0.0229 |                2.5024 |      1.4812 |        59.6453 |\n",
      "|      46|                  34|        2951.4595|                 0.0117 |                2.5145 |      1.5425 |        61.2279 |\n",
      "|      47|                  37|        2951.8514|                 0.0133 |                2.5281 |      1.2831 |        62.5506 |\n",
      "|      48|                  38|        2951.8541|                 0.0001 |                2.5282 |      1.5965 |        64.1874 |\n",
      "|      49|                  34|        2951.8772|                 0.0008 |                2.5290 |      1.6420 |        65.8702 |\n",
      "|      50|                  37|        2951.8837|                 0.0002 |                2.5292 |      1.4088 |        67.3201 |\n",
      "|      51|                  44|        2951.9297|                 0.0016 |                2.5308 |      2.2407 |        69.6008 |\n",
      "|      52|                  41|        2951.9340|                 0.0001 |                2.5310 |      1.2643 |        70.9103 |\n",
      "|      53|                  45|        2952.0499|                 0.0039 |                2.5350 |      1.3699 |        72.3212 |\n",
      "|      54|                  44|        2952.1088|                 0.0020 |                2.5370 |      1.2894 |        73.6502 |\n",
      "|      55|                  45|        2952.2728|                 0.0056 |                2.5427 |      1.4199 |        75.1104 |\n",
      "|      56|                  42|        2952.2730|                 0.0000 |                2.5427 |      1.5265 |        76.6760 |\n",
      "|      57|                  42|        2952.2730|                 0.0000 |                2.5427 |      1.6403 |        78.3596 |\n",
      "|      58|                  44|        2952.3556|                 0.0028 |                2.5456 |      1.2414 |        79.6401 |\n",
      "|      59|                  46|        2952.3998|                 0.0015 |                2.5471 |      1.7399 |        81.4187 |\n",
      "|      60|                  35|        2952.4785|                 0.0027 |                2.5499 |      1.5628 |        83.0220 |\n",
      "|      61|                  35|        2954.4997|                 0.0684 |                2.6201 |      1.5123 |        84.5986 |\n",
      "|      62|                  36|        2954.6701|                 0.0058 |                2.6260 |      1.3270 |        85.9680 |\n",
      "|      63|                  38|        2956.2128|                 0.0522 |                2.6796 |      1.7642 |        87.7714 |\n",
      "|      64|                  36|        2956.3166|                 0.0035 |                2.6832 |      1.3584 |        89.1702 |\n",
      "|      65|                  44|        2956.4044|                 0.0030 |                2.6862 |      1.3565 |        90.5660 |\n",
      "|      66|                  44|        2956.4602|                 0.0019 |                2.6882 |      1.7817 |        92.3859 |\n",
      "|      67|                  43|        2956.5193|                 0.0020 |                2.6902 |      1.5123 |        93.9386 |\n",
      "|      68|                  37|        2956.9228|                 0.0136 |                2.7042 |      1.4469 |        95.4250 |\n",
      "|      69|                  41|        2957.0032|                 0.0027 |                2.7070 |      1.4175 |        96.8828 |\n",
      "|      70|                  45|        2957.0989|                 0.0032 |                2.7104 |      1.5687 |        98.4915 |\n",
      "|      71|                  41|        2957.1173|                 0.0006 |                2.7110 |      1.3740 |        99.9074 |\n",
      "|      72|                  36|        2957.1304|                 0.0004 |                2.7115 |      1.6466 |       101.5981 |\n",
      "|      73|                  42|        2957.2884|                 0.0053 |                2.7169 |      1.6467 |       103.2839 |\n",
      "|      74|                  44|        2957.3817|                 0.0032 |                2.7202 |      1.9691 |       105.2943 |\n",
      "|      75|                  42|        2957.4071|                 0.0009 |                2.7211 |      1.4021 |       106.7371 |\n",
      "|      76|                  43|        2957.4606|                 0.0018 |                2.7229 |      1.4126 |       108.1885 |\n",
      "|      77|                  41|        2957.9284|                 0.0158 |                2.7392 |      2.1063 |       110.3363 |\n",
      "|      78|                  41|        2958.0249|                 0.0033 |                2.7425 |      1.4502 |       111.8288 |\n",
      "|      79|                  42|        2958.2728|                 0.0084 |                2.7511 |      1.4407 |       113.3101 |\n",
      "|      80|                  42|        2959.0928|                 0.0277 |                2.7796 |      1.9653 |       115.3149 |\n",
      "|      81|                  41|        2960.2158|                 0.0379 |                2.8186 |      1.7140 |       117.0941 |\n",
      "|      82|                  42|        2960.2993|                 0.0028 |                2.8215 |      1.3630 |       118.4963 |\n",
      "|      83|                  43|        2960.3693|                 0.0024 |                2.8240 |      1.3736 |       119.9132 |\n",
      "|      84|                  42|        2960.3946|                 0.0009 |                2.8248 |      1.3243 |       121.2769 |\n",
      "|      85|                  43|        2960.4111|                 0.0006 |                2.8254 |      1.4738 |       122.7892 |\n",
      "|      86|                  38|        2960.5783|                 0.0056 |                2.8312 |      1.9528 |       124.7826 |\n",
      "|      87|                  39|        2960.6187|                 0.0014 |                2.8326 |      1.5644 |       126.3864 |\n",
      "|      88|                  42|        2961.1432|                 0.0177 |                2.8508 |      1.4155 |       127.8430 |\n",
      "|      89|                  43|        2961.6145|                 0.0159 |                2.8672 |      1.4032 |       129.2862 |\n",
      "|      90|                  44|        2961.6466|                 0.0011 |                2.8683 |      1.5984 |       130.9248 |\n",
      "|      91|                  47|        2962.1756|                 0.0179 |                2.8867 |      1.5132 |       132.4782 |\n",
      "|      92|                  51|        2962.8455|                 0.0226 |                2.9100 |      1.7655 |       134.2873 |\n",
      "|      93|                  53|        2962.9215|                 0.0026 |                2.9126 |      1.8523 |       136.1812 |\n",
      "|      94|                  46|        2963.0071|                 0.0029 |                2.9156 |      1.6871 |       137.9096 |\n",
      "|      95|                  48|        2963.0371|                 0.0010 |                2.9166 |      1.6115 |       139.5649 |\n",
      "|      96|                  50|        2963.0838|                 0.0016 |                2.9182 |      1.3594 |       140.9650 |\n",
      "|      97|                  51|        2963.2251|                 0.0048 |                2.9231 |      1.2053 |       142.2093 |\n",
      "|      98|                  51|        2963.3274|                 0.0035 |                2.9267 |      1.4033 |       143.6514 |\n",
      "|      99|                  52|        2963.4026|                 0.0025 |                2.9293 |      1.3861 |       145.0775 |\n",
      "|     100|                  52|        2963.5196|                 0.0039 |                2.9334 |      1.3175 |       146.4343 |\n",
      "---------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "instanceName = '50v-10'\n",
    "cpt = NaiveCuttingPlaneTreeAlgorithm(instanceName,training=True, save_interval=20, maxIteration=100, OutputFlag = 0, Threads = 1, MIPGap = 0.0, TimeLimit = 300, number_branch_var = 2, normalization = 'SNC')\n",
    "cpt.train_model_each_iteration()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OptML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
